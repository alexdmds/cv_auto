{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"cv_generator\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/q2b2x49j16x2jfrqxg8_grdc0000gn/T/ipykernel_11755/2630718512.py:15: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"mistral\", temperature=0, base_url=\"http://127.0.0.1:11500\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse GPT-4o mini: content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-2721a673-503f-4113-bd5b-89b436fc886f-0' usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import datetime\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "# Récupérer la clé API OpenAI depuis les variables d'environnement\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"La clé API OpenAI n'est pas définie dans les variables d'environnement.\")\n",
    "\n",
    "# Modèle local Mistral via Ollama\n",
    "llm = ChatOllama(model=\"mistral\", temperature=0, base_url=\"http://127.0.0.1:11500\")\n",
    "\n",
    "# Modèle GPT-4o via OpenAI\n",
    "llm2 = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    api_key=openai_api_key  # Utilisation de la clé API OpenAI depuis l'env\n",
    ")\n",
    "\n",
    "# Test des modèles\n",
    "#print(\"Réponse Mistral:\", llm.invoke(\"Hello, world!\"))  # Mistral\n",
    "print(\"Réponse GPT-4o mini:\", llm2.invoke(\"Hello, world!\"))  # GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import du texte source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe le texte et on en extrait les principales expériences de façon destructuré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "# Charger le contenu du fichier source\n",
    "source_file = Path(\"source_brut.txt\")\n",
    "if not source_file.exists():\n",
    "    raise FileNotFoundError(\"Le fichier source_brut.txt est introuvable.\")\n",
    "\n",
    "with source_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    source_text = f.read()\n",
    "\n",
    "time_start = datetime.datetime.now()\n",
    "total_tokens = 0\n",
    "prompt_tokens = 0\n",
    "completion_tokens = 0\n",
    "total_cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Étape 1 : Génération de la liste des expériences et formations\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    À partir du texte suivant qui décrit un profil candidat :\n",
    "    \"\"\"\n",
    "    \"{source}\"\n",
    "    \"\"\"\n",
    "    Extrait une liste concise de ses expériences et formations sous la forme suivante :\n",
    "    - Expérience : Intitulé, Dates, Lieu\n",
    "    - Éducation : Intitulé, Dates, Lieu\n",
    "    \"\"\"\n",
    ")\n",
    "summary_chain = summary_prompt | llm2 | StrOutputParser()\n",
    "\n",
    "# Exécution de la chaîne en capturant le coût via le callback\n",
    "with get_openai_callback() as cb:\n",
    "    summary_text = summary_chain.invoke({\"source\": source_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transormation en JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "\n",
    "# Définir la structure de données souhaitée\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    intitule: str = Field(description=\"Intitulé du poste ou de l'expérience\")\n",
    "    dates: str = Field(description=\"Période de l'expérience\")\n",
    "    etablissement: str = Field(description=\"Nom de l'établissement\")\n",
    "    lieu: str = Field(description=\"Lieu de l'expérience\")\n",
    "\n",
    "class Education(BaseModel):\n",
    "    intitule: str = Field(description=\"Intitulé du diplôme ou de la formation\")\n",
    "    dates: str = Field(description=\"Période de la formation\")\n",
    "    etablissement: str = Field(description=\"Nom de l'établissement\")\n",
    "    lieu: str = Field(description=\"Lieu de l'établissement\")\n",
    "\n",
    "class CV(BaseModel):\n",
    "    experiences: List[Experience] = Field(description=\"Liste des expériences professionnelles\")\n",
    "    education: List[Education] = Field(description=\"Liste des formations\")\n",
    "\n",
    "# Instancier le parser JSON avec le modèle CV\n",
    "parser = JsonOutputParser(pydantic_object=CV)\n",
    "\n",
    "# Créer un prompt en injectant les instructions de format fournies par le parser\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Transforme le texte suivant en un JSON structuré avec deux clés : \"experiences\" et \"education\".\n",
    "Le format attendu est :\n",
    "{format_instructions}\n",
    "\n",
    "Texte :\n",
    "\"{summary}\"\n",
    "\"\"\",\n",
    "    input_variables=[\"summary\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Construire la chaîne en combinant prompt, modèle de langage et parser\n",
    "chain = prompt | llm2 | parser\n",
    "\n",
    "# Exécution de la chaîne en capturant le coût via le callback\n",
    "with get_openai_callback() as cb1:\n",
    "    json_output = chain.invoke({\"summary\": summary_text})\n",
    "\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichissement des exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion du JSON en dictionnaire\n",
    "parsed_json = json_output\n",
    "\n",
    "# Étape 3 : Ajout d'une description à chaque expérience et formation\n",
    "description_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Voici le profil détaillé du candidat :\n",
    "    \"{source}\"\n",
    "    \n",
    "    Basé sur ces informations, rédige une description extrêmement détaillée et exhaustive de l'expérience suivante, en incluant **toutes** les informations disponibles dans le texte source :\n",
    "    \n",
    "    - Intitulé : {intitule}\n",
    "    - Dates : {dates}\n",
    "    - Établissement : {etablissement}\n",
    "    - Lieu : {lieu}\n",
    "    \n",
    "    Intègre les responsabilités, les missions précises, les compétences mises en œuvre, les résultats obtenus, les outils et technologies utilisées, les collaborations, les projets majeurs associés, les distinctions éventuelles et toute autre information pertinente.\n",
    "    \n",
    "    Donne uniquement la description enrichie en sortie, sans aucun autre texte additionnel.\n",
    "    Ne rappelle pas les informations sur les dates, l'établissement ou le lieu.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def enrich_with_description(entry):\n",
    "    global total_tokens, prompt_tokens, completion_tokens, total_cost\n",
    "    description_chain = description_prompt | llm2 | StrOutputParser()\n",
    "\n",
    "    # Exécution de la chaîne en capturant le coût via le callback\n",
    "    with get_openai_callback() as cb2:\n",
    "        description = description_chain.invoke({\"source\": source_text, **entry})\n",
    "    \n",
    "    total_tokens += cb.total_tokens\n",
    "    prompt_tokens += cb.prompt_tokens\n",
    "    completion_tokens += cb.completion_tokens\n",
    "    total_cost += cb.total_cost\n",
    "\n",
    "    entry[\"description\"] = description\n",
    "    return entry\n",
    "\n",
    "parsed_json[\"experiences\"] = [enrich_with_description(exp) for exp in parsed_json[\"experiences\"]]\n",
    "parsed_json[\"education\"] = [enrich_with_description(edu) for edu in parsed_json[\"education\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du temps d'exécution\n",
    "time_diff = datetime.datetime.now() - time_start\n",
    "execution_time_sec = time_diff.total_seconds()\n",
    "\n",
    "# Ajout des informations de coût dans le JSON\n",
    "parsed_json[\"total_tokens\"] = cb.total_tokens + cb1.total_tokens + total_tokens\n",
    "parsed_json[\"prompt_tokens\"] = cb.prompt_tokens + cb1.prompt_tokens + prompt_tokens\n",
    "parsed_json[\"completion_tokens\"] = cb.completion_tokens + cb1.completion_tokens + completion_tokens\n",
    "parsed_json[\"total_cost\"] = cb.total_cost + cb1.total_cost + total_cost\n",
    "\n",
    "\n",
    "#on ajoute les clés au json\n",
    "parsed_json[\"source_file\"] = source_file.name\n",
    "parsed_json[\"date\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "parsed_json[\"method\"] = \"modulaire\"\n",
    "parsed_json[\"model\"] = \"gpt-4o-mini\"\n",
    "parsed_json[\"execution_time\"] = execution_time_sec\n",
    "\n",
    "\n",
    "# Sauvegarde du résultat dans un fichier JSON avec le timestamp\n",
    "output_file = Path(f\"output_mod_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.json\")\n",
    "with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(parsed_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"JSON enrichi avec descriptions et sauvegardé dans output.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
