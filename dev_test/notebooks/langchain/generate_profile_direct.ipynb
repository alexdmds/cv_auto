{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"cv_generator\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "time_start = datetime.datetime.now()\n",
    "\n",
    "# Définir le modèle Pydantic pour la structure attendue\n",
    "type Experiences = list[dict[str, str]]\n",
    "type Education = list[dict[str, str]]\n",
    "\n",
    "class ProfileData(BaseModel):\n",
    "    experiences: Experiences = Field(description=\"Liste des expériences professionnelles du candidat\")\n",
    "    education: Education = Field(description=\"Liste des formations académiques du candidat\")\n",
    "\n",
    "# Initialisation du modèle LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Définir le parser JSON\n",
    "parser = JsonOutputParser(pydantic_object=ProfileData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "# Charger le contenu du fichier source\n",
    "source_file = Path(\"source_brut.txt\")\n",
    "if not source_file.exists():\n",
    "    raise FileNotFoundError(\"Le fichier source_brut.txt est introuvable.\")\n",
    "\n",
    "with source_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    source_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "# Création du prompt en intégrant les instructions de formatage\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Analyse le texte suivant décrivant un profil candidat et génère un JSON structuré.\n",
    "    Le JSON doit contenir deux clés : \"experiences\" et \"education\", avec les champs suivants :\n",
    "    \n",
    "    - Pour chaque expérience :\n",
    "      - \"intitule\": Intitulé du poste\n",
    "      - \"dates\": Période d'emploi\n",
    "      - \"etablissement\": Nom de l'entreprise\n",
    "      - \"lieu\": Localisation\n",
    "      - \"description\": L'intégralité des informations disponibles concernant cette expérience, sans résumé ni reformulation.\n",
    "    \n",
    "    - Pour chaque formation :\n",
    "      - \"intitule\": Nom du diplôme\n",
    "      - \"dates\": Période de formation\n",
    "      - \"etablissement\": Nom de l'institution\n",
    "      - \"lieu\": Localisation\n",
    "      - \"description\": L'intégralité des informations disponibles sur cette formation, sans résumé ni reformulation.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \n",
    "    Texte source :\n",
    "    \"\"\"\n",
    "    \"{source}\"\n",
    "    \"\"\"\n",
    "    \"\"\",\n",
    "    input_variables=[\"source\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Exécuter le prompt avec le modèle\n",
    "json_chain = prompt | llm | parser\n",
    "\n",
    "\n",
    "# Exécution de la chaîne en capturant le coût via le callback\n",
    "with get_openai_callback() as cb:\n",
    "    json_output = json_chain.invoke({\"source\": source_text})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON structuré et enrichi généré avec succès et sauvegardé dans output.json\n"
     ]
    }
   ],
   "source": [
    "# Ajout des informations de coût dans le JSON\n",
    "json_output[\"total_tokens\"] = cb.total_tokens\n",
    "json_output[\"prompt_tokens\"] = cb.prompt_tokens\n",
    "json_output[\"completion_tokens\"] = cb.completion_tokens\n",
    "json_output[\"total_cost\"] = cb.total_cost\n",
    "\n",
    "# Sauvegarde du résultat dans un fichier JSON\n",
    "\n",
    "# Calcul du temps d'exécution\n",
    "time_diff = datetime.datetime.now() - time_start\n",
    "execution_time_sec = time_diff.total_seconds()\n",
    "\n",
    "\n",
    "#on ajoute des informations supplémentaires sur le run\n",
    "#on ajoute le nom du fichier texte source, la date et l'heure, \"direct\" (le nom de cette méthode) et le nom du modèle utilisé\n",
    "json_output[\"source_file\"] = source_file.name\n",
    "json_output[\"date\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "json_output[\"method\"] = \"direct\"\n",
    "json_output[\"model\"] = \"gpt-4o-mini\"\n",
    "json_output[\"execution_time\"] = execution_time_sec\n",
    "\n",
    "output_file = Path(f\"output_dir_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.json\")\n",
    "with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"JSON structuré et enrichi généré avec succès et sauvegardé dans output.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
